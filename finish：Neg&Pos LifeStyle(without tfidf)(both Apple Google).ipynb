{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\") # the stopwords \n",
    "def remain_english(line):\n",
    "    rule =re.compile(\"[^a-z^A-Z]\")\n",
    "    line = rule.sub('',line)\n",
    "    return line\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v')) # remain problems\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(str(text)) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "#             result.append(lemmatize_stemming(token))\n",
    "            result.append(remain_english(lemmatize_stemming((token))).lower()) # take out not english and return into verb, and lower it \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (4,5,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read file into pandas\n",
    "import pandas as pd\n",
    "fileGP = r'C:\\Users\\Administrator\\Desktop\\reviewData\\Reviews(GP)\\LifeStyleReview.csv'\n",
    "fileApple = r'C:\\Users\\Administrator\\Desktop\\reviewData\\Reviews(Ap)\\lifestylereviews.csv'\n",
    "# file = r'C:\\Users\\Administrator\\Desktop\\reviewData\\dist\\lifestyle-dist.csv'\n",
    "df1 = pd.read_csv(fileGP,error_bad_lines=False)#  Error tokenizing data. C error: Expected 13 fields in line 8791, saw 25\n",
    "df2 = pd.read_csv(fileApple,encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of reviews text: 109212\n",
      "number of neg: 33328\n",
      "number of inconsistant neg reviews text(actually a pos reviews): 178\n",
      "number of pos: 67267\n",
      "number of inconsistant pos reviews text(actually an neg reviews): 143\n"
     ]
    }
   ],
   "source": [
    "gpNum = df1.count()['text']\n",
    "ApNum = df2.count()['text'] # text(reviews) number from apple\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "text_num = gpNum + ApNum\n",
    "# text_num = 5500\n",
    "documentNeg=[]\n",
    "documentPos=[]\n",
    "inconsistantNum1 = 0 # inconsistant reviews number in low rating reviews\n",
    "inconsistantNum2 = 0\n",
    "for i in range(gpNum):\n",
    "    if(df1.score[i]<3): # low rating reviews\n",
    "#         print(sid.polarity_scores(df1.text[i])['neg'])\n",
    "        if(sid.polarity_scores(str(df1.text[i]))['pos'] > 0.5): # 如果是这条评论的sensitive分析分数显示这条评论其实是负面的,threhold = 0.5\n",
    "            inconsistantNum1 += 1\n",
    "        else:\n",
    "            documentNeg.append(df1.text[i])             \n",
    "\n",
    "    if(df1.score[i]>3): # high rating reviews\n",
    "        if(sid.polarity_scores(str(df1.text[i]))['neg'] > 0.5): # if the neg score is high, not use this review,threhold = 0.5\n",
    "            inconsistantNum2 += 1\n",
    "        else:\n",
    "            documentPos.append(df1.text[i]) \n",
    "            \n",
    "for j in range(ApNum):\n",
    "    if(df2.score[j]<3): # low rating reviews\n",
    "        if(sid.polarity_scores(str(df2.text[j]))['pos'] > 0.5): # 如果是这条评论的sensitive分析分数显示这条评论其实是负面的,threhold = 0.5\n",
    "            inconsistantNum1 += 1\n",
    "        else:\n",
    "            documentNeg.append(df2.text[j])             \n",
    "    if(df2.score[j]>3): # low rating reviews\n",
    "        if(sid.polarity_scores(str(df2.text[j]))['neg'] > 0.5): # if the neg score is high, not use this review,threhold = 0.5\n",
    "            inconsistantNum2 += 1\n",
    "        else:\n",
    "            documentPos.append(df2.text[j])      \n",
    "\n",
    "print('number of reviews text:',text_num)\n",
    "print('number of neg:',len(documentNeg))\n",
    "print('number of inconsistant neg reviews text(actually a pos reviews):',inconsistantNum1)\n",
    "print('number of pos:',len(documentPos))\n",
    "print('number of inconsistant pos reviews text(actually an neg reviews):',inconsistantNum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made doc_term_matrix\n",
      "object num 33328\n",
      "Fitting Time: 2689.7548451423645\n",
      "Topic: 0 \n",
      "Words: 0.038*\"game\" + 0.031*\"hate\" + 0.027*\"crap\" + 0.025*\"worst\" + 0.025*\"play\" + 0.024*\"stupid\" + 0.020*\"scan\" + 0.018*\"piec\" + 0.017*\"junk\" + 0.015*\"question\"\n",
      "Topic: 1 \n",
      "Words: 0.021*\"servic\" + 0.020*\"custom\" + 0.018*\"time\" + 0.014*\"point\" + 0.012*\"month\" + 0.012*\"card\" + 0.011*\"week\" + 0.011*\"money\" + 0.011*\"order\" + 0.011*\"support\"\n",
      "Topic: 2 \n",
      "Words: 0.035*\"time\" + 0.033*\"wast\" + 0.028*\"watch\" + 0.027*\"money\" + 0.027*\"free\" + 0.025*\"don\" + 0.020*\"download\" + 0.019*\"want\" + 0.016*\"like\" + 0.016*\"work\"\n",
      "Topic: 3 \n",
      "Words: 0.058*\"work\" + 0.044*\"updat\" + 0.034*\"open\" + 0.032*\"crash\" + 0.029*\"time\" + 0.020*\"camera\" + 0.018*\"load\" + 0.016*\"iphon\" + 0.016*\"screen\" + 0.011*\"great\"\n",
      "Topic: 4 \n",
      "Words: 0.036*\"like\" + 0.025*\"look\" + 0.023*\"peopl\" + 0.020*\"list\" + 0.020*\"search\" + 0.014*\"color\" + 0.014*\"pictur\" + 0.013*\"fake\" + 0.012*\"profil\" + 0.012*\"want\"\n",
      "Topic: 5 \n",
      "Words: 0.037*\"connect\" + 0.037*\"work\" + 0.026*\"bulb\" + 0.019*\"updat\" + 0.019*\"light\" + 0.018*\"time\" + 0.015*\"turn\" + 0.013*\"devic\" + 0.010*\"control\" + 0.009*\"product\"\n",
      "Topic: 6 \n",
      "Words: 0.036*\"account\" + 0.026*\"email\" + 0.025*\"tri\" + 0.024*\"sign\" + 0.024*\"say\" + 0.023*\"password\" + 0.022*\"login\" + 0.022*\"messag\" + 0.022*\"time\" + 0.015*\"error\"\n",
      "Topic: 7 \n",
      "Words: 0.018*\"user\" + 0.015*\"function\" + 0.014*\"need\" + 0.013*\"like\" + 0.013*\"version\" + 0.012*\"featur\" + 0.011*\"better\" + 0.011*\"design\" + 0.010*\"develop\" + 0.008*\"interfac\"\n",
      "Topic: 8 \n",
      "Words: 0.026*\"phone\" + 0.024*\"toyota\" + 0.022*\"locat\" + 0.021*\"work\" + 0.017*\"updat\" + 0.017*\"time\" + 0.014*\"vehicl\" + 0.013*\"need\" + 0.011*\"pandora\" + 0.010*\"servic\"\n",
      "Topic: 9 \n",
      "Words: 0.144*\"star\" + 0.099*\"\" + 0.042*\"rat\" + 0.030*\"zero\" + 0.023*\"hair\" + 0.021*\"rate\" + 0.019*\"chore\" + 0.016*\"kid\" + 0.011*\"onstar\" + 0.010*\"give\"\n"
     ]
    }
   ],
   "source": [
    "# negtive reviews analyse\n",
    "document = documentNeg\n",
    "doc_clean = [preprocess(doc) for doc in document]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# 使用上面的词典，将转换文档列表（语料）变成 DT 矩阵\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "print('made doc_term_matrix')\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "print('object num',len(doc_term_matrix))\n",
    "start = time.time()\n",
    "lda_model = gensim.models.LdaMulticore(doc_term_matrix, num_topics=10, id2word=dictionary, passes=100, workers=2)\n",
    "# result = lda_model.print_topics(num_words=10)\n",
    "end = time.time()\n",
    "print ('Fitting Time:',end-start)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object num 67267\n",
      "Fitting Time: 4131.502308368683\n",
      "Topic: 0 \n",
      "Words: 0.071*\"\" + 0.017*\"chore\" + 0.016*\"heat\" + 0.015*\"spin\" + 0.013*\"coffe\" + 0.012*\"auto\" + 0.012*\"prize\" + 0.012*\"relax\" + 0.011*\"okay\" + 0.010*\"case\"\n",
      "Topic: 1 \n",
      "Words: 0.026*\"time\" + 0.023*\"easi\" + 0.021*\"great\" + 0.019*\"money\" + 0.018*\"recommend\" + 0.016*\"servic\" + 0.015*\"point\" + 0.014*\"high\" + 0.012*\"card\" + 0.010*\"price\"\n",
      "Topic: 2 \n",
      "Words: 0.051*\"work\" + 0.029*\"great\" + 0.018*\"wallpap\" + 0.016*\"home\" + 0.016*\"light\" + 0.015*\"control\" + 0.013*\"easi\" + 0.012*\"connect\" + 0.012*\"watch\" + 0.012*\"devic\"\n",
      "Topic: 3 \n",
      "Words: 0.025*\"like\" + 0.020*\"color\" + 0.018*\"star\" + 0.017*\"updat\" + 0.017*\"time\" + 0.015*\"work\" + 0.015*\"review\" + 0.012*\"camera\" + 0.010*\"thing\" + 0.009*\"problem\"\n",
      "Topic: 4 \n",
      "Words: 0.033*\"help\" + 0.019*\"know\" + 0.018*\"time\" + 0.018*\"need\" + 0.017*\"great\" + 0.016*\"home\" + 0.016*\"locat\" + 0.016*\"search\" + 0.014*\"love\" + 0.014*\"famili\"\n",
      "Topic: 5 \n",
      "Words: 0.181*\"good\" + 0.103*\"great\" + 0.072*\"nice\" + 0.033*\"free\" + 0.030*\"like\" + 0.023*\"stuff\" + 0.018*\"pretti\" + 0.014*\"music\" + 0.012*\"lot\" + 0.011*\"choos\"\n",
      "Topic: 6 \n",
      "Words: 0.020*\"event\" + 0.017*\"shade\" + 0.016*\"plan\" + 0.015*\"wed\" + 0.014*\"mile\" + 0.014*\"timer\" + 0.012*\"walk\" + 0.011*\"guest\" + 0.010*\"shirt\" + 0.010*\"restaur\"\n",
      "Topic: 7 \n",
      "Words: 0.048*\"like\" + 0.045*\"peopl\" + 0.034*\"cool\" + 0.024*\"best\" + 0.024*\"awesom\" + 0.023*\"date\" + 0.023*\"app\" + 0.020*\"friend\" + 0.018*\"look\" + 0.018*\"game\"\n",
      "Topic: 8 \n",
      "Words: 0.031*\"love\" + 0.025*\"read\" + 0.022*\"help\" + 0.022*\"thank\" + 0.016*\"life\" + 0.014*\"feel\" + 0.013*\"daili\" + 0.011*\"thing\" + 0.011*\"time\" + 0.011*\"accur\"\n",
      "Topic: 9 \n",
      "Words: 0.167*\"love\" + 0.055*\"easi\" + 0.029*\"great\" + 0.025*\"amaz\" + 0.024*\"pictur\" + 0.019*\"photo\" + 0.015*\"like\" + 0.014*\"super\" + 0.013*\"look\" + 0.013*\"make\"\n"
     ]
    }
   ],
   "source": [
    "# positive\n",
    "document = documentPos\n",
    "doc_clean = [preprocess(doc) for doc in document]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "# 使用上面的词典，将转换文档列表（语料）变成 DT 矩阵\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "print('object num',len(doc_term_matrix))\n",
    "start = time.time()\n",
    "lda_model = gensim.models.LdaMulticore(doc_term_matrix, num_topics=10, id2word=dictionary, passes=100, workers=2)\n",
    "# result = lda_model.print_topics(num_words=10)\n",
    "end = time.time()\n",
    "print ('Fitting Time:',end-start)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
